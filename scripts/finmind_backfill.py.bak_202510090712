# -*- coding: utf-8 -*-
# FinMind Backfill (Strict Fix)
# 修復點：
# 1) 單股資料集（prices/chip/per/dividend）「是否已覆蓋」改為按 stock_id+date 判斷
# 2) 新增 --force 可強制打 API（跳過 gating）
# 3) --datasets 支援 alias：prices|chip|per|dividend
# 4) symbols 可空白或逗號分隔；若未給則讀 investable_universe.txt 或 universe.tw_all.txt
# 5) 寫入銀層：datahub/silver/alpha/{prices|chip|per|dividend}/yyyymm=YYYYMM/*.parquet

import os, sys, json, time, math, argparse, glob, datetime
from urllib import request, parse, error
import pandas as pd

# --------------------- 基本環境 ---------------------
BASE = os.environ.get("FINMIND_BASE_URL", "https://api.finmindtrade.com/api/v4/data")
TOKEN = (os.environ.get("FINMIND_TOKEN") or "").strip()
if not TOKEN:
    print("ERROR: FINMIND_TOKEN 未設定", file=sys.stderr); sys.exit(2)

def alias_to_dataset(name: str) -> str:
    m = {
        "prices":"TaiwanStockPrice", "price":"TaiwanStockPrice", "taiwanstockprice":"TaiwanStockPrice",
        "chip":"TaiwanStockInstitutionalInvestorsBuySell", "institutional":"TaiwanStockInstitutionalInvestorsBuySell",
        "per":"TaiwanStockPER", "taiwanstockper":"TaiwanStockPER",
        "dividend":"TaiwanStockDividend", "taiwanstockdividend":"TaiwanStockDividend"
    }
    k = (name or "").strip().lower()
    return m.get(k, name)

def dataset_to_kind(ds: str) -> str:
    if ds.endswith("TaiwanStockPrice"): return "prices"
    if ds.endswith("InstitutionalInvestorsBuySell"): return "chip"
    if ds.endswith("TaiwanStockPER"): return "per"
    if ds.endswith("TaiwanStockDividend"): return "dividend"
    # fallback：用 alias 名
    a = ds.lower()
    if "price" in a: return "prices"
    if "buysell" in a or "institutional" in a: return "chip"
    if "per" in a: return "per"
    if "dividend" in a: return "dividend"
    return "prices"

def parse_date(s: str) -> datetime.date:
    return datetime.datetime.strptime(s, "%Y-%m-%d").date()

def http_get(dataset: str, data_id: str, start: str, end: str) -> pd.DataFrame:
    qs = parse.urlencode({
        "dataset": dataset,
        "data_id": data_id,
        "start_date": start,
        "end_date": end
    })
    req = request.Request(f"{BASE}?{qs}", headers={"Authorization": f"Bearer {TOKEN}"})
    with request.urlopen(req, timeout=30) as r:
        obj = json.loads(r.read().decode("utf-8"))
        data = obj.get("data") or []
        if not data:
            return pd.DataFrame()
        df = pd.DataFrame(data)
        # 標準化
        if "date" in df.columns:
            df["date"] = pd.to_datetime(df["date"], errors="coerce").dt.strftime("%Y-%m-%d")
        if "stock_id" in df.columns:
            df["stock_id"] = df["stock_id"].astype(str)
        # 補 symbol（去 .TW）
        df["symbol"] = df.get("stock_id", "").astype(str).str.replace(".TW","", regex=False)
        return df

def yyyymm_of(date_str: str) -> str:
    d = parse_date(date_str)
    return f"{d.year:04d}{d.month:02d}"

def silver_has_symbol_day(root: str, kind: str, symbol: str, day: str) -> bool:
    """僅掃該日所在月與前一月的分區，查是否已有該 symbol+day 的列"""
    base = os.path.join(root, "silver", "alpha", kind)
    ym = yyyymm_of(day)
    ym_prev_date = (parse_date(day).replace(day=1) - datetime.timedelta(days=1))
    ym_prev = f"{ym_prev_date.year:04d}{ym_prev_date.month:02d}"
    pats = [
        os.path.join(base, f"yyyymm={ym}", "**", "*.parquet"),
        os.path.join(base, f"yyyymm={ym_prev}", "**", "*.parquet"),
    ]
    files = []
    for p in pats: files.extend(glob.glob(p, recursive=True))
    if not files: return False
    for f in files:
        try:
            df = pd.read_parquet(f, columns=["date","stock_id"])
            if df.empty: 
                continue
            s = df["stock_id"].astype(str).str.replace(".TW","", regex=False)
            if ((df["date"].astype(str) == day) & (s == symbol)).any():
                return True
        except Exception:
            pass
    return False

def write_silver(df: pd.DataFrame, root: str, kind: str) -> int:
    if df is None or df.empty: 
        return 0
    df = df.copy()
    df["yyyymm"] = pd.to_datetime(df["date"], errors="coerce").dt.strftime("%Y%m")
    total = 0
    for ym, g in df.groupby("yyyymm"):
        outdir = os.path.join(root, "silver", "alpha", kind, f"yyyymm={ym}")
        os.makedirs(outdir, exist_ok=True)
        out = os.path.join(outdir, f"ing_{kind}_{ym}_{int(time.time()*1000)}.parquet")
        g.drop(columns=["yyyymm"], errors="ignore").to_parquet(out, index=False)
        total += len(g)
    return total

def load_pool(root="."):
    # 優先 investable_universe.txt -> universe.tw_all.txt
    for p in [os.path.join(root, "configs", "investable_universe.txt"),
              os.path.join(root, "universe.tw_all.txt")]:
        if os.path.exists(p):
            syms = []
            with open(p, "r", encoding="utf-8", errors="ignore") as fh:
                for line in fh:
                    x = line.strip().replace(".TW","")
                    if x and len(x)==4 and x.isdigit():
                        syms.append(x)
            return sorted(set(syms))
    return []

def chunked(seq, n):
    for i in range(0, len(seq), n):
        yield seq[i:i+n]

# --------------------- 主程式 ---------------------
def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--datasets", nargs="+", required=True, help="datasets or alias (prices/chip/per/dividend)")
    ap.add_argument("--symbols", nargs="*", help="symbols (空白或逗號分隔)")
    ap.add_argument("--start", required=True)
    ap.add_argument("--end", required=True)         # 不含
    ap.add_argument("--datahub-root", default="datahub")
    ap.add_argument("--force", action="store_true", help="強制打 API（跳過覆蓋判斷）")
    args = ap.parse_args()

    # 正規化 datasets
    ds_list = []
    for token in args.datasets:
        for part in (token.split(",") if "," in token else [token]):
            ds_list.append(alias_to_dataset(part))
    ds_list = list(dict.fromkeys(ds_list))  # 去重並保序

    # 正規化 symbols
    syms = []
    if args.symbols:
        for s in args.symbols:
            for p in (s.split(",") if "," in s else [s]):
                q = p.strip().replace(".TW","")
                if q and len(q)==4 and q.isdigit():
                    syms.append(q)
        syms = list(sorted(set(syms)))

    # 無 symbols 時，讀 pool
    if not syms:
        syms = load_pool(".")
    universe = "TSE" if syms else "N/A"

    # Header
    print("=== FinMind Backfill (API, Strict Fix) ===")
    print(f"Start={args.start} End={args.end} Universe={universe}")
    print(f"Datasets={','.join([dataset_to_kind(d) for d in ds_list])}")
    mode = ("單股 指定 %d 檔" % len(syms)) if syms else "全市場（由本地投資池決定）"
    print(f"Mode={mode}")
    # 簡單節流（環境變數）
    qps = float(os.environ.get("FINMIND_QPS","1.5"))
    sleep_sec = max(0.0, 1.0/ qps)
    print(f"QPS={qps:.3f}")

    end_minus1 = (parse_date(args.end) - datetime.timedelta(days=1)).strftime("%Y-%m-%d")

    total_metrics = []
    for ds in ds_list:
        kind = dataset_to_kind(ds)
        rows_written = 0
        files_out = 0
        sink_dir = os.path.join(args.datahub_root, "silver", "alpha", kind)

        # 準備 symbols_to_fetch（按 symbol+day 判斷）
        todo = list(syms)
        if not args.force and syms:
            todo2 = []
            for s in syms:
                need = not silver_has_symbol_day(args.datahub_root, kind, s, end_minus1)
                if need:
                    todo2.append(s)
            todo = todo2

        estcalls = len(todo) if syms else 0  # 沒有 pool 時不做全市場盲打
        print(f"== Phase: {kind}  EstCalls={estcalls}")
        if syms and not todo and not args.force:
            print(f"⚠️  {kind} symbols 全部判定已覆蓋（{end_minus1}）。若需重打，請加 --force 或移除覆蓋判斷。")
            total_metrics.append(dict(dataset=kind, mode=mode, estcalls=0, rows_written=0, files_out=0, sink_dir=sink_dir))
            continue

        # 逐檔打 API
        if syms:
            for s in todo:
                try:
                    df = http_get(ds, s, args.start, args.end)
                    if df is not None and not df.empty:
                        w = write_silver(df, args.datahub_root, kind)
                        rows_written += w
                        files_out += 1 if w>0 else 0
                    time.sleep(sleep_sec)
                except Exception as e:
                    print(f"[WARN] {kind} {s}: {e}", file=sys.stderr)
        else:
            # 沒 symbols → 讀 pool；若 pool 仍空，跳過（避免盲打全市場）
            pool = load_pool(".")
            if not pool:
                print(f"⚠️  無 symbols 且無投資池，略過 {kind}")
            else:
                for batch in chunked(pool, 400):
                    for s in batch:
                        try:
                            df = http_get(ds, s, args.start, args.end)
                            if df is not None and not df.empty:
                                w = write_silver(df, args.datahub_root, kind)
                                rows_written += w
                                files_out += 1 if w>0 else 0
                            time.sleep(sleep_sec)
                        except Exception as e:
                            print(f"[WARN] {kind} {s}: {e}", file=sys.stderr)

        print(f"OK {kind}: rows_written={rows_written} files_out={files_out}")
        total_metrics.append(dict(dataset=kind, mode=mode, estcalls=estcalls, rows_written=rows_written, files_out=files_out, sink_dir=sink_dir))

    # 輸出 metrics
    os.makedirs("metrics", exist_ok=True)
    ts = datetime.datetime.now().strftime("%Y%m%d-%H%M%S")
    outcsv = os.path.join("metrics", f"ingest_summary_{ts}_finmind.csv")
    pd.DataFrame(total_metrics).to_csv(outcsv, index=False, encoding="utf-8")
    print(f"=== Backfill Done ===  metrics: {os.path.abspath(outcsv)}")

if __name__ == "__main__":
    # Windows codepage
    try:
        import sys
        if hasattr(sys.stdout, "reconfigure"):
            sys.stdout.reconfigure(encoding="utf-8")
    except Exception:
        pass
    main()
