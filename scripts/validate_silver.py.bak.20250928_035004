#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
強化版銀層驗證器：
- 以 schema.yaml 驅動，支援多型別白名單、可選欄位、額外欄位容忍
- --strict 模式才檢查 required 欄位的 NA 數量
- 產出 CSV 報告（dataset, file, status, rows, missing_cols, type_errors, na_count）
使用方式：
python scripts/validate_silver.py --datahub-root <PATH> --schema-path <YAML> --report-csv <CSV> [--strict]
"""
import argparse
import os
import sys
import glob
import json
import pandas as pd

try:
    import yaml
except Exception as e:
    print("缺少套件 pyyaml，請先安裝：pip install pyyaml", file=sys.stderr)
    raise

def _dtype_key(dtype_obj) -> str:
    """標準化 dtype 字串（處理 pandas string/可空整數/時區）"""
    d = str(dtype_obj)
    # 正規化一些常見別名
    if d.lower().startswith("datetime64[ns, "):
        return "datetime64[ns, UTC]"  # 統一成帶時區的寫法
    if d.lower().startswith("datetime64[ns]"):
        return "datetime64[ns]"
    if d.lower().startswith("string"):
        return "string[python]" if d == "string[python]" else "string"
    return d

def _check_types(df: pd.DataFrame, col_types: dict) -> dict:
    """回傳各欄位型別是否通過與實際 dtype"""
    errors = {}
    for col, rule in (col_types or {}).items():
        if col not in df.columns:
            continue  # 型別僅在欄位存在時才檢
        actual = _dtype_key(df[col].dtype)
        allowed = set(rule.get("any_of", []))
        # 允許 object 當作後備（文字類型常見）
        if "object" in allowed and actual == "object":
            ok = True
        else:
            ok = actual in allowed
        if not ok:
            errors[col] = {"actual": actual, "allowed": sorted(list(allowed))}
    return errors

def _na_required(df: pd.DataFrame, required_cols: list) -> dict:
    na = {}
    for col in (required_cols or []):
        if col in df.columns:
            na[col] = int(df[col].isna().sum())
        else:
            na[col] = None  # 缺欄位另外在 missing_cols 通報
    return na

def validate_file(file_path: str, dataset_name: str, schema: dict, strict: bool) -> dict:
    required = schema.get("required_columns", []) or []
    optional = set(schema.get("optional_columns", []) or [])
    col_types = schema.get("column_types", {}) or {}
    allow_extra = bool(schema.get("allow_extra_columns", True))

    try:
        df = pd.read_parquet(file_path, engine="pyarrow")
    except Exception as e:
        return {
            "dataset": dataset_name,
            "file": file_path,
            "status": "FAIL",
            "rows": None,
            "missing_cols": "read_error",
            "type_errors": f"{e.__class__.__name__}: {e}",
            "na_count": None,
        }

    cols = set(df.columns)
    missing = [c for c in required if c not in cols]

    # 額外欄位允許 → 不做任何處理；若未來要封鎖可在此比較
    type_errs = _check_types(df, col_types)
    na_req = _na_required(df, required)

    status = "OK"
    reasons = []
    if missing:
        status = "FAIL"
        reasons.append(f"missing={missing}")
    if type_errs:
        status = "FAIL"
        reasons.append(f"type={list(type_errs.keys())}")
    if strict:
        # 在嚴格模式下才用 required 欄位 NA 觸發 FAIL
        na_bad = {k: v for k, v in na_req.items() if isinstance(v, int) and v > 0}
        if na_bad:
            status = "FAIL"
            reasons.append(f"na_required={list(na_bad.keys())}")

    return {
        "dataset": dataset_name,
        "file": file_path,
        "status": status if not reasons else f"{status} ({';'.join(reasons)})",
        "rows": int(len(df)),
        "missing_cols": ",".join(missing) if missing else "",
        "type_errors": json.dumps(type_errs, ensure_ascii=False) if type_errs else "",
        "na_count": json.dumps(na_req, ensure_ascii=False),
    }

def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--datahub-root", required=True, help="DataHub 根目錄（例如 G:\AI\tw-alpha-stack\datahub）")
    ap.add_argument("--schema-path", required=True, help="YAML Schema 路徑")
    ap.add_argument("--report-csv", required=True, help="輸出報告 CSV 路徑")
    ap.add_argument("--strict", action="store_true", help="嚴格模式：required 欄位 NA>0 視為 FAIL")
    args = ap.parse_args()

    import yaml
    with open(args.schema_path, "r", encoding="utf-8") as f:
        schema = yaml.safe_load(f)

    paths = schema.get("paths", []) or []
    results = []

    for entry in paths:
        name = entry.get("name")
        rel_glob = entry.get("glob")
        if not name or not rel_glob:
            continue
        pattern = os.path.join(args.datahub_root, rel_glob).replace("\\", "/")
        files = glob.glob(pattern, recursive=True)
        for fp in sorted(files):
            if not fp.lower().endswith(".parquet"):
                continue
            r = validate_file(fp, name, schema, strict=args.strict)
            results.append(r)

    os.makedirs(os.path.dirname(args.report_csv), exist_ok=True)
    pd.DataFrame(results).to_csv(args.report_csv, index=False, encoding="utf-8-sig")

    # 顯示摘要於 stdout
    if results:
        ok = sum(1 for r in results if str(r["status"]).startswith("OK"))
        fail = len(results) - ok
        print(f"[SUMMARY] files={len(results)} OK={ok} FAIL={fail}")
        if fail:
            print("前 5 個 FAIL：")
            for r in [r for r in results if not str(r['status']).startswith('OK')][:5]:
                import os as _os
                print(f"- {r['dataset']} | {_os.path.basename(r['file'])} | {r['status']}")
    else:
        print("未找到任何 parquet 檔，請確認 --datahub-root 與 schema.paths 設定")

if __name__ == "__main__":
    main()
