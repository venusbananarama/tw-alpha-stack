
#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
build_universe.py - FinMind investable universe builder (dataset-specific, rigorous)

What it does
------------
1) Fetches TaiwanStockInfo from FinMind.
2) Filters to a clean investable set (by universe TSE/OTC/ALL).
   - Excludes ETFs/ETNs/REITs, bonds, warrants/beneficiary certificates,
     non 4-digit stock IDs, and obvious non-common shares.
3) Writes meta outputs under <datahub_root>/_meta:
   - investable_universe.csv         (general investable list)
   - investable_prices.csv           (for prices dataset)
   - investable_chip.csv             (for chip dataset; optionally probed)
   - non_investable_excluded.csv     (excluded + reason, for audit)
   - symbols.txt                     (one symbol per line with suffix, e.g. 2330.TW)
   - universe_summary.json           (counts + paths)
4) Optional "chip coverage probe": call chip dataset for recent days to keep
   only symbols that actually have institutional data, avoiding empty hits later.
   This probe runs behind a rate limiter and caches to file; repeated runs will
   only probe newly-listed symbols unless --force-probe is set.

CLI
---
python build_universe.py --universe TSE --datahub-root <path> [--include-etf]
                         [--probe-chip-days 7] [--qps 1.5] [--hourly-cap 6000]
                         [--api-token <token>] [--force-probe]

Token order: --api-token > env FINMIND_TOKEN > secrets/finmind_token.txt
"""

from __future__ import annotations

import argparse
import json
import os
import re
import time
from datetime import date, timedelta
from pathlib import Path
from typing import Dict, Iterable, List, Optional, Tuple

import pandas as pd

try:
    import requests  # type: ignore
except Exception:  # pragma: no cover
    requests = None  # type: ignore

API_V4 = "https://api.finmindtrade.com/api/v4/data"
PROJECT_ROOT = Path(__file__).resolve().parents[1]

# ----------------------------- Utilities -----------------------------

def ensure_dir(p: Path) -> None:
    p.mkdir(parents=True, exist_ok=True)


def today_str() -> str:
    return date.today().strftime("%Y-%m-%d")


def dminus(days: int) -> str:
    return (date.today() - timedelta(days=days)).strftime("%Y-%m-%d")


def get_token(cli: Optional[str]) -> Optional[str]:
    if cli:
        return cli
    v = os.environ.get("FINMIND_TOKEN")
    if v:
        return v
    f = PROJECT_ROOT / "secrets" / "finmind_token.txt"
    if f.exists():
        return f.read_text(encoding="utf-8").strip()
    return None


class RateLimiter:
    """Global limiter: QPS + hourly cap (best-effort)."""
    def __init__(self, qps: float, hourly_cap: int):
        from collections import deque
        self.calls = deque()
        self.qps = max(0.1, float(qps))
        self.hourly_cap = int(hourly_cap)
        self.last_ts = 0.0
        cap_qps = (self.hourly_cap / 3600.0) * 0.95  # 5% headroom
        if self.qps > cap_qps:
            self.qps = cap_qps

    def acquire(self):
        now = time.time()
        gap = 1.0 / self.qps
        wait = self.last_ts + gap - now
        if wait > 0:
            time.sleep(wait)
            now = time.time()
        while self.calls and now - self.calls[0] > 3600.0:
            self.calls.popleft()
        if len(self.calls) >= self.hourly_cap:
            time.sleep(3600.0 - (now - self.calls[0]) + 0.05)
            return self.acquire()
        self.calls.append(now)
        self.last_ts = now


def http_get(params: Dict[str, str], rl: Optional[RateLimiter] = None, timeout: float = 45.0) -> Dict:
    if requests is None:  # pragma: no cover
        raise RuntimeError("requests is not installed. Please: pip install requests")
    if rl:
        rl.acquire()
    r = requests.get(API_V4, params=params, timeout=timeout)
    if r.status_code != 200:
        raise RuntimeError(f"HTTP {r.status_code}: {r.text[:200]}")
    return r.json()


# ----------------------------- Fetchers -----------------------------

def fetch_stock_info(token: Optional[str]) -> pd.DataFrame:
    params = {"dataset": "TaiwanStockInfo"}
    if token:
        params["token"] = token
    data = http_get(params)
    return pd.DataFrame(data.get("data", []))


def probe_chip_has_data(sid: str, token: Optional[str], days: int, rl: RateLimiter) -> bool:
    """Return True if chip dataset has rows for stock within last `days` calendar days."""
    params = {
        "dataset": "TaiwanStockInstitutionalInvestorsBuySell",
        "start_date": dminus(days),
        "end_date": today_str(),
        "stock_id": sid,
    }
    if token:
        params["token"] = token
    try:
        data = http_get(params, rl=rl)
        return bool(data.get("data"))
    except Exception:
        return False


# ----------------------------- Filters -----------------------------

EXC_PATTERNS = [
    re.compile(r"ETF|ETN|REIT|WARRANT|權證|受益|受益憑證", re.I),
    re.compile(r"債|公司債|國債|公債|連結債|票據|票券", re.I),
    re.compile(r"\bDR\b|存託憑證", re.I),
]

def attach_symbol(stock_id: str, universe: str, market: str) -> str:
    uni = universe.upper()
    if uni == "TSE":
        return f"{stock_id}.TW"
    if uni == "OTC":
        return f"{stock_id}.TWO"
    # ALL: infer from market string
    return f"{stock_id}.TWO" if ("OTC" in market.upper() or "上櫃" in market) else f"{stock_id}.TW"


def filter_investable(df: pd.DataFrame, universe: str, include_etf: bool = False) -> Tuple[pd.DataFrame, pd.DataFrame]:
    """Return (investable, excluded_with_reason)."""
    uni = universe.upper()
    x = df.copy()

    # universe filter
    if "market" in x.columns:
        m = x["market"].astype(str).str.upper()
        if uni == "TSE":
            x = x[m.str.contains("TSE|上市", na=False)]
        elif uni == "OTC":
            x = x[m.str.contains("OTC|上櫃", na=False)]
        else:
            x = x[m.str.contains("TSE|上市|OTC|上櫃", na=False)]

    keep_rows = []
    excl_rows = []
    for _, r in x.iterrows():
        sid = str(r.get("stock_id", "")).strip()
        sname = str(r.get("stock_name", "")).strip()
        typ = str(r.get("type", "")).strip()
        market = str(r.get("market", "")).strip()

        reason = ""
        if not sid.isdigit() or len(sid) != 4:
            reason = "non-4digit"
        elif sid.startswith("00") and not include_etf:
            reason = "leading-00(ETF-like)"
        else:
            for pat in EXC_PATTERNS:
                if pat.search(typ) or pat.search(sname):
                    reason = f"type/name match: {pat.pattern}"
                    break

        if reason:
            excl_rows.append({"stock_id": sid, "stock_name": sname, "market": market, "type": typ, "exclude_reason": reason})
        else:
            keep_rows.append({"stock_id": sid, "stock_name": sname, "market": market, "type": typ})

    keep = pd.DataFrame(keep_rows)
    excl = pd.DataFrame(excl_rows)
    if not keep.empty:
        keep["symbol"] = keep.apply(lambda r: attach_symbol(r["stock_id"], uni, r.get("market", "")), axis=1)
    return keep, excl


# ----------------------------- Builder -----------------------------

def build_universe(universe: str, token: Optional[str], datahub_root: Path,
                   include_etf: bool, probe_chip_days: int,
                   qps: float, hourly_cap: int, force_probe: bool) -> Dict[str, str]:
    ensure_dir(datahub_root / "_meta")
    meta = datahub_root / "_meta"

    # 1) base info
    info = fetch_stock_info(token)
    if info.empty:
        raise RuntimeError("TaiwanStockInfo is empty; check token or API availability.")

    # 2) general investable filter
    inv, excl = filter_investable(info, universe, include_etf=include_etf)

    # 3) dataset-specific
    # prices: all investable are OK
    inv_prices = inv.copy()

    # chip: cached run (only probe delta unless forced)
    chip_cache_path = meta / "investable_chip.csv"
    if chip_cache_path.exists() and not force_probe:
        prev = pd.read_csv(chip_cache_path, encoding="utf-8")
        prev_ids = set(prev["stock_id"].astype(str).tolist())
    else:
        prev_ids = set()

    need_probe = [sid for sid in inv["stock_id"].astype(str).tolist() if sid not in prev_ids]
    rl = RateLimiter(qps=qps, hourly_cap=hourly_cap)

    ok_ids: List[str] = sorted(prev_ids) if prev_ids else []
    if need_probe or force_probe or not chip_cache_path.exists():
        ok = []
        for sid in need_probe if not force_probe else inv["stock_id"].astype(str).tolist():
            if probe_chip_has_data(sid, token, probe_chip_days, rl):
                ok.append(sid)
        ok_ids = sorted(set(ok_ids).union(ok))

    inv_chip = inv[inv["stock_id"].astype(str).isin(ok_ids)].copy()

    # 4) write outputs
    paths = {}
    inv.to_csv(meta / "investable_universe.csv", index=False, encoding="utf-8")
    paths["investable_universe"] = str(meta / "investable_universe.csv")

    inv_prices.to_csv(meta / "investable_prices.csv", index=False, encoding="utf-8")
    paths["investable_prices"] = str(meta / "investable_prices.csv")

    inv_chip.to_csv(chip_cache_path, index=False, encoding="utf-8")
    paths["investable_chip"] = str(chip_cache_path)

    if not excl.empty:
        excl.to_csv(meta / "non_investable_excluded.csv", index=False, encoding="utf-8")
        paths["non_investable_excluded"] = str(meta / "non_investable_excluded.csv")

    # symbols.txt (from general investable)
    with (meta / "symbols.txt").open("w", encoding="utf-8") as f:
        for s in inv["symbol"].astype(str).tolist():
            f.write(s + "\n")
    paths["symbols_txt"] = str(meta / "symbols.txt")

    # summary json
    summary = {
        "universe": universe,
        "generated_at": today_str(),
        "counts": {
            "info_total": int(len(info)),
            "investable": int(len(inv)),
            "excluded": int(len(excl)),
            "prices": int(len(inv_prices)),
            "chip": int(len(inv_chip)),
        },
        "paths": paths,
    }
    (meta / "universe_summary.json").write_text(json.dumps(summary, ensure_ascii=False, indent=2), encoding="utf-8")
    paths["universe_summary"] = str(meta / "universe_summary.json")

    return paths


# ----------------------------- CLI -----------------------------

def main() -> int:
    ap = argparse.ArgumentParser()
    ap.add_argument("--universe", default="TSE", choices=["TSE", "OTC", "ALL"])
    ap.add_argument("--datahub-root", default=str(PROJECT_ROOT / "datahub"))
    ap.add_argument("--api-token", default=None)
    ap.add_argument("--include-etf", action="store_true")
    ap.add_argument("--probe-chip-days", type=int, default=7, help="Probe recent calendar days for chip coverage.")
    ap.add_argument("--qps", type=float, default=1.5)
    ap.add_argument("--hourly-cap", type=int, default=6000)
    ap.add_argument("--force-probe", action="store_true", help="Re-probe all investable symbols for chip coverage.")
    args = ap.parse_args()

    token = get_token(args.api_token)
    if not token:
        print("❌ Missing FinMind token. Use --api-token or FINMIND_TOKEN environment.", file=sys.stderr)
        return 2

    paths = build_universe(
        args.universe, token, Path(args.datahub_root),
        include_etf=args.include_etf, probe_chip_days=args.probe_chip_days,
        qps=args.qps, hourly_cap=args.hourly_cap, force_probe=args.force_probe
    )
    print("=== Universe built (dataset-specific) ===")
    for k, v in paths.items():
        print(f"{k}: {v}")
    return 0


if __name__ == "__main__":
    raise SystemExit(main())
